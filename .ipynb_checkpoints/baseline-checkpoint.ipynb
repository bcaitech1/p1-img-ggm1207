{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "intermediate-thesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "statistical-shareware",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "musical-gamma",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/opt/ml/input/data'\n",
    "\n",
    "train_path = os.path.join(data_dir, \"train/images\")\n",
    "test_path = os.path.join(data_dir, \"eval/images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "qualified-ballet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5400, 25200)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(train_path)), len(os.listdir(test_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hundred-politics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'._005231_male_Asian_19'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(train_path)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "representative-quilt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'._006164_female_Asian_19'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(train_path)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "molecular-ebony",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "os.chdir('/opt/ml/P-Stage/1-STAGE/')\n",
    "\n",
    "def get_classes(key):\n",
    "    \"\"\" predict하기 위해서는 순서가 중요하다. \"\"\"\n",
    "    if key == \"mask\":\n",
    "        return [\"wear\", \"incorrect\", \"not wear\"]\n",
    "    if key == \"age\":\n",
    "        return [\"age < 30\", \"30 <= age < 60\", \"60 <= age\"]\n",
    "    if key == \"gender\":\n",
    "        return [\"male\", \"female\"]\n",
    "    raise KeyError(\"key must be in ['mask', 'age', 'gender']\")\n",
    "\n",
    "\n",
    "def get_transforms(args):\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((args.image_size, args.image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return transform\n",
    "\n",
    "\n",
    "class MaskDataSet(Dataset):\n",
    "    def __init__(self, args, is_train=True, transform=None):\n",
    "        csv_file = os.path.join(args.data_dir, \"train.csv\")\n",
    "        self.datas = pd.read_csv(csv_file)\n",
    "        self.images, self.labels = self._load_image_files_path(args, is_train)\n",
    "        self.label_idx = [\"gender\", \"age\", \"mask\"].index(args.train_key)\n",
    "\n",
    "        if args.test:\n",
    "            self.images, self.labels = self.images[:100], self.labels[:100]\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.images[idx])\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, self.labels[idx][self.label_idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def _load_image_files_path(self, args, is_train):\n",
    "        split = StratifiedShuffleSplit(\n",
    "            n_splits=1,\n",
    "            test_size=args.valid_size,\n",
    "            random_state=0,  # 이 SEED값은 안 바꾸는 것이 좋다.\n",
    "        )\n",
    "\n",
    "        split_key = \"age\" if args.train_key == \"age\" else \"gender\"\n",
    "        \n",
    "        for train_index, valid_index in split.split(self.datas, self.datas[split_key]):\n",
    "            train_dataset = self.datas.loc[train_index]\n",
    "            valid_dataset = self.datas.loc[valid_index]\n",
    "\n",
    "        dataset = train_dataset if is_train else valid_dataset\n",
    "        gender_classes = get_classes(\"gender\")\n",
    "\n",
    "        images = []\n",
    "        labels = []\n",
    "\n",
    "        for dir_name in dataset[\"path\"]:\n",
    "            dir_path = os.path.join(args.data_dir, \"images\", dir_name)\n",
    "\n",
    "            image_id, gender_lbl, _, age_lbl = dir_name.split(\"_\")\n",
    "\n",
    "            gender_class = gender_classes.index(gender_lbl)\n",
    "\n",
    "            # [\"age < 30\", \"30 <= age < 60\", \"60 <= age\"]\n",
    "            age_lbl = int(age_lbl)\n",
    "\n",
    "            if age_lbl < 30:\n",
    "                age_class = 0\n",
    "            elif age_lbl >= 60:\n",
    "                age_class = 2\n",
    "            else:\n",
    "                age_class = 1\n",
    "\n",
    "            for jpg_filepath in glob(dir_path + \"/*\"):\n",
    "                jpg_basename = os.path.basename(jpg_filepath)\n",
    "\n",
    "                if \"normal\" in jpg_basename:\n",
    "                    mask_class = 2\n",
    "                elif \"incorrect\" in jpg_basename:\n",
    "                    mask_class = 1\n",
    "                else:\n",
    "                    mask_class = 0\n",
    "\n",
    "                images.append(jpg_filepath)\n",
    "                labels.append((gender_class, age_class, mask_class))\n",
    "\n",
    "        return images, labels\n",
    "\n",
    "\n",
    "def get_dataloader(args):\n",
    "    transform = get_transforms(args)\n",
    "\n",
    "    train_dataset = MaskDataSet(args, is_train=True, transform=transform)\n",
    "    valid_dataset = MaskDataSet(args, is_train=False, transform=transform)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=args.workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    valid_dataloader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=args.workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    return train_dataloader, valid_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cognitive-wells",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2700"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset.datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laughing-accuracy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-matter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "significant-directory",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import get_args\n",
    "\n",
    "args = get_args()\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = MaskDataSet(args, is_train=True, transform=transform)\n",
    "valid_dataset = MaskDataSet(args, is_train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-robert",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-knife",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-torture",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-steal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-bosnia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alike-brush",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "mounted-queensland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15120, 3780)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "boolean-snapshot",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, lbl = next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "technical-automation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<function Tensor.size>, (1, 56, 0))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.size, lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "caroline-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_img_list = glob(data_dir + \"/train/images/*/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "qualified-harvard",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18900"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_img_list) # folder: 2700, jpg: 18900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "published-former",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/ml/input/data/train/images/000523_female_Asian_51/mask4.jpg'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_img_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "muslim-hunter",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, lbl = next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "promotional-knowing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((384, 512), (1, 56, 0))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.size, lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "extreme-basketball",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "strange-sensitivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "willing-reduction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 3, 224, 224]),\n",
       " [tensor([0, 1, 0, 1, 0, 0, 0, 0]),\n",
       "  tensor([19, 19, 25, 19, 25, 36, 56, 21]),\n",
       "  tensor([0, 0, 0, 0, 0, 0, 0, 1])])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "planned-forum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-innocent",
   "metadata": {},
   "source": [
    "![](https://s3-ap-northeast-2.amazonaws.com/aistages-public-junyeop/app/Users/00000025/files/56bd7d05-4eb8-4e3e-884d-18bd74dc4864..png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "subsequent-channels",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = [0, 1, 2]\n",
    "gender = [0, 1]\n",
    "age = [0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "mental-reporter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0\n",
      "0\n",
      "0 0 1\n",
      "1\n",
      "0 0 2\n",
      "2\n",
      "0 1 0\n",
      "3\n",
      "0 1 1\n",
      "4\n",
      "0 1 2\n",
      "5\n",
      "1 0 0\n",
      "6\n",
      "1 0 1\n",
      "7\n",
      "1 0 2\n",
      "8\n",
      "1 1 0\n",
      "9\n",
      "1 1 1\n",
      "10\n",
      "1 1 2\n",
      "11\n",
      "2 0 0\n",
      "12\n",
      "2 0 1\n",
      "13\n",
      "2 0 2\n",
      "14\n",
      "2 1 0\n",
      "15\n",
      "2 1 1\n",
      "16\n",
      "2 1 2\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "def get_class(mi, gi, ai):\n",
    "    return 6 * mi + 3 * gi + ai\n",
    "\n",
    "for mi in mask:\n",
    "    for gi in gender:\n",
    "        for ai in age:\n",
    "            print(mi, gi, ai)\n",
    "            print(get_class(mi, gi, ai))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-badge",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
